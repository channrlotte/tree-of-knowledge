ВЕСТНИК ПЕРМСКОГО НАУЧНОГО ЦЕНТРА 1/2014
УДК 004.272
А.В. Созыкин,
Институт математики и механики им. Н.Н. Красовского УрО РАН
М.Л. Гольдштейн,
Институт математики и механики им. Н.Н. Красовского УрО РАН
Выполнен обзор направлений деятельности суперкомпьютерного центра Института математики и механики им. Н.Н. Красовского УрО РАН. Описаны имеющиеся вычислительные ресурсы и перспективы их развития. Рассматриваются вопросы создания распределенной ГРИД-инфраструктуры УрО РАН. Представлен подход к созданию облачной платформы для вычислений на суперкомпьютерах. Описана образовательная деятельность в области параллельных вычислений и суперкомпьютерных технологий.
Ключевые слова: суперкомпьютеры, параллельные вычисления, облачные вычисления, Big Data, ГРИД, распределенные вычисления.
ВВЕДЕНИЕ
Институт математики и механики им. Н.Н. Красовского УрО РАН (ИММ УрО РАН) является головной организацией по созданию и развитию вычислительных ресурсов в УрО РАН. В связи с этим в ИММ УрО РАН действует суперкомпьютерный центр (СКЦ), задачей которого является обеспечение научно-образовательного сообщества Уральского региона современным вычислительным инструментарием мирового уровня. В настоящее время наиболее мощным вычислительным ресурсом СКЦ ИММ УрО РАН является суперкомпьютер (СК) «УРАН» [5].
Другим направлением деятельности СКЦ ИММ УрО РАН является создание
распределенной ГРИД-инфраструктуры УрО РАН, которая должна включать вычислительные ресурсы и системы хранения не только в Екатеринбурге, но и в других регионах присутствия УрО РАН [1]. В настоящее время создан первый региональный вычислительный центр в Институте механики сплошных сред УрО РАН (ИМСС УрО РАН), г. Пермь.
В целях повышения эффективности применения параллельных вычислительных ресурсов пользователями создается облачная платформа с пакетами прикладных программ для суперкомпьютерных вычислений [2] и обработки больших объемов данных.
Образовательная деятельность в области параллельных вычислений и супер-
* Работа выполнена при финансовой поддержке УрО РАН, проект 12-П-1-2012, а также Правительства Свердловской области и РФФИ, проект 13-07-96006 р_урал_а.
компьютерных технологий ведется на базовой кафедре ИММ УрО РАН в Институте математики и компьютерных наук Уральского федерального университета (УрФУ) «Высокопроизводительные компьютерные технологии».
ВЫЧИСЛИТЕЛЬНЫЕ РЕСУРСЫ
СКЦ ИММ УрО РАН предоставляет пользователям ряд вычислительных ресурсов, наиболее мощным из которых является суперкомпьютер (СК) «УРАН». СК был создан в 2008 г., последняя модернизация завершилась в 2013 г. В результате модернизации пиковая производительность СК «УРАН» была увеличена до 225 TFlops (миллионов операций с плавающей точкой в секунду), производительность на тесте Linpack [18] составила 109 TFlops. Это позволило СК «УРАН» войти в список TOP 500 наиболее мощных суперкомпьютеров мира [19] (позиция номер 428 в редакции списка от июня 2013 г.) и в список Green 500 [20] наиболее энергетически эффективных суперкомпьютеров мира (позиция номер 385 в редакции списка от июня 2013 г.).
СК «УРАН» использует гибридную архитектуру: для вычисления применяются не только центральные процессоры (CPU, Central Processing Unit), но и специализированные графические ускорители вычислений (GPU, Graphics processing unit). Отличительной особенностью GPU является большое количество вычислительных ядер (сотни и тысячи), работающих параллельно, в то время как современные CPU содержат не более 16 ядер. При этом GPU имеют упрощенный набор поддерживаемых операций и меньший размер КЭШа по сравнению с CPU. Однако за счет большого числа вычислительных ядер производительность GPU превосходит производительность CPU в несколько раз. Также GPU обладает более высокой энергетической эффективностью по сравнению с CPU. Именно этот факт позволил СК «УРАН» занять значительно более высокую позицию в списке энерго-
эффективных СК Gren 500 по сравнению с общим списком TOP 500.
СК «УРАН» содержит специализированные графические ускорители для проведения высокопроизводительных вычислений компании NVIDIA серии Tesla в конструктивном исполнении для установки в серверное оборудование. В настоящее время в состав СК «УРАН» входят 208 GPU Tesla M2090 (производительность 665 GFlops, 512 вычислительных ядер, 6 ГБ памяти) и 160 GPU NVIDIA Tesla M2050 (производительность 512 GFlops, 448 вычислительных ядер, 3 ГБ памяти).
Существует несколько способов применения GPU для проведения параллельных вычислений. Наиболее простой способ - использование готового приложения с поддержкой GPU. В настоящее время разработано несколько десятков таких приложений [15] для разных предметных областей, среди которых много бесплатных. Кроме бесплатных пакетов, на СК «УРАН» установлены коммерческие пакеты MATLAB и Ansys, которые также поддерживают вычисления на GPU. Второй способ - разработка собственных программ на основе готовых математических библиотек для GPU или технологии OpenACC [17], обеспечивающей возможность переноса кода на GPU без больших изменений программы с использованием директив компилятора. Это позволяет быстро получить работающую программу, однако ее производительность может быть далека от оптимальной. Третий способ - разработка специализированной программы для GPU с использованием технологии NVIDIA CUDA (Compute Unified Device Architecture) [6] - расширения языка С. Технология CUDA предоставляет максимальный набор средств для повышения производительности вычислений на GPU, но разработка требует значительного времени.
Пользователи СК «УРАН» чаще всего применяют готовые бесплатные пакеты для молекулярной динамики.
Система хранения СК «УРАН» по-
строена на основе дискового массива EMC Celerra NS-480, полная емкость которого составляет 50 ТБ (диски SATA). Подключение системы хранения к СК «УРАН» выполняется по сети Gigabit Ethernet с использованием протокола NFS.
Среди задач, решаемых на СК «УРАН», можно отметить следующие:
-	математическое моделирование сердца;
-	расчет оптимальной траектории вывода полезной нагрузки ракетоносителями «Союз-2» и «Русь-М» на эллиптическую орбиту;
-	исследование информативности геофизических полей для навигации автономных движущихся объектов;
-	моделирование динамики и сейсмичности литосферы;
-	численное моделирование взаимодействия излучения с веществом;
-	разработка параллельных алгоритмов решения СЛАУ с блочными матрицами для задач электроразведки и др.
В дальнейшем предполагается развитие вычислительных ресурсов путем увеличения их производительности в целях устойчивого пребывания в списке TOP 500 наиболее мощных суперкомпьютеров мира.
Новые СК в СКЦ ИММ УрО РАН планируется создавать на основе гибридной архитектуры с использованием ускорителей. До недавнего времени в качестве ускорителей использовались только GPU, но сейчас компания Intel разработала новый тип ускорителя вычислений: Intel MIC (Many Integrated Core) на основе стандартной архитектуры CPU Intel. Преимуществом этого ускорителя является простота адаптации программы, т.к. ускоритель использует ту же архитектуру, что и CPU. Однако в настоящее время ускорители Intel MIC поддерживаются малым количеством приложений, что может существенно ограничить их применение пользователями СКЦ ИММ УрО РАН. Выбор типа ускорителей вычислений для будущих СК является открытым вопросом в развитии СКЦ ИММ УрО РАН.
РАСПРЕДЕЛЕННАЯ ГРИД-ИНФРАСТРУКТУРА УрО РАН
В настоящее время общепризнанным фактом является невозможность создания одного крупного СКЦ, способного обслуживать любые задачи любых пользователей, независимо от их местоположения. Для этого не хватит ресурсов, как финансовых, так и технических (ограничения на площади вычислительных залов, доступные мощности электропитания, возможности охлаждения). Эффективное решение крупных научных задач требует объединения вычислительных ресурсов разных организаций по высокоскоростным каналам связи, которое реализуется с помощью ГРИД-технологий. Наиболее известным примером является ГРИД-ин-фраструктура Большого адронного кол-лайдера WLCG [21], в рамках которой объединены вычислительные ресурсы многих стран Европы и США. Россия также принимает участие в этом проекте.
В УрО РАН ведутся работы по созданию распределенной ГРИД-инфраструк-туры, которая будет охватывать регионы присутствия отделения. В настоящее время в СКЦ ИММ УрО РАН разработана концептуальная модель ГРИД УрО РАН [1] и начаты работы по ее реализации.
Первый региональный вычислительный центр (РВЦ) создан в ИМСС УрО РАН, г. Пермь. В РВЦ передана часть оборудования СКЦ ИММ УрО РАН, ранее входившая в состав СК «УРАН»: три шасси HP Blade c7000, 48 серверов-лезвий HP BL 460c, коммутатор Infiniband DDR Voltair. Пиковая производительность переданного вычислительного кластера 4,5 TFlops. Также в РВЦ передан сервер хранения данных Supermicro, дисковая емкость которого составляет 24 ТБ.
Подключение РВЦ ИМСС УрО РАН к СКЦ ИММ УрО РАН выполнено по высокоскоростному каналу связи Пермь-Екатеринбург, построенному в рамках проекта Giga UrB RAS [4]. Использованы оптические линии связи и оборудование DWDM. В настоящее время доступны 2
канала по 10 Гб/c и 8 каналов по 1 Гб/с, для ГРИД-инфраструктуры используется один канал 10 Гб/с.
Текущая схема ГРИД-инфраструкту-ры УрО РАН показана на рис. 1.
Основные компоненты ГРИД-инфра-структуры: вычислительные ресурсы и системы хранения. Вычислительные ресурсы включают СК «УРАН» и вычислительный кластер в РВЦ ИМСС УрО РАН, интегрированные друг с другом. Распределенная система хранения построена на основе независимых серверов хранения данных Supermicro. Три таких сервера установлено в СКЦ ИММ УрО РАН и один в РВЦ ИМСС УрО РАН. Объединение серверов в логически единую систему хранения выполнено с помощью промежуточного программного обеспечения dCache [14]. Отличительной особенностью dCache является поддержка не только ГРИД-протоколов, но и стандартных протоколов Интернет для распределенных систем хранения, в частности Parallel NFS и WebDAV. Это значительно облегчило подключение распределенной системы хранения к вычислительному оборудованию, которое было выполнено по протоколу NFS версии 4.1 (составной частью которого и является Parallel NFS). Клиент NFS версии 4.1 встроен в исполь-
зуемую на вычислительных узлах операционную систему Linux, в результате на вычислительном оборудовании не пришлось ничего дополнительно устанавливать и поддерживать. С другой стороны, обеспечена возможность в дальнейшем подключить распределенную систему хранения к Российским и международным ГРИД-инфраструктурам по протоколам ГРИД.
К ресурсам РВЦ ИМСС УрО РАН подключены экспериментальные установки ИМСС УрО РАН, генерирующие в процессе работы большие объемы данных и требующие много вычислительных ресурсов для обработки экспериментальных данных в целях получения новых научных результатов. В качестве примера экспериментальных установок можно привести систему PIV (Particle Image Velocimetry) [9] и стенд для измерения факела форсунки, используемой в авиадвигателях [7].
В дальнейшем планируется создавать РВЦ в других крупных региональных научных центрах УрО РАН: Ижевске, Сыктывкаре, Архангельске. Для расширения возможностей научного взаимодействия ГРИД-инфраструктуру УрО РАН планируется подключить к российским и мировым ГРИД-инфраструктурам, таким как
Рис. 1. Схема текущей реализации ГРИД УрО РАН
ГРИД Российской Федерации [10], создаваемый под руководством Минсвязи РФ, ГРИД национальной нанотехнологиче-ской сети (ГРИД ННС) [3], международная ГРИД-инфраструктура WLCG [21] (через ее Российский сегмент РДИГ [8]).
ОБЛАЧНАЯ ПЛАТФОРМА
ДЛЯ ВЫЧИСЛЕНИЙ НА СУПЕРКОМПЬЮТЕРАХ
Эффективность применения СК можно повысить путем предоставления пользователям знакомого им предметно-ориентированного пакета прикладных программ, который поддерживает параллельные вычисления. В этом случае пользователи могут начать работать на СК с минимальными затратами на изучение технологий параллельных вычислений.
Однако пакеты прикладных программ, поддерживающие параллельные вычисления, отличаются высокой сложностью в установке и настройке. Обычным пользователям сложно самостоятельно установить такой пакет и настроить его на работу с СК. Решением является технология облачных вычислений, при которой пользователи работают с программным обеспечением, установленным не на их компьютерах, а на серверах поставщиков облачных услуг.
Предлагается предоставлять пользователям СКЦ ИММ УрО РАН пакеты прикладных программ для параллельных вычислений по модели SaaS [2]. Пакеты устанавливаются на серверах вычислительной облачной платформы в СКЦ ИММ УрО РАН профессиональными администраторами и интегрируются с СК «УРАН». Под интеграцией понимается возможность запуска задачи на СК прямо из графического интерфейса пакета с последующей обработкой результатов расчетов (анализ данных, визуализация и т.п.) также в графическом интерфейсе. Пользователям на свои компьютеры устанавливать ничего не нужно, требуется только Web-браузер.
Схема архитектуры облачной платформы для вычислений на суперкомпьютерах приведена на рис. 2.
В настоящее время выполнена тестовая реализация облачной платформы. Установлен пакет прикладных программ MATLAB с расширением Parallel Computing Toolbox для проведения параллельных вычислений на удаленном вычислительном кластере (используется СК «УРАН»). Также облачная платформа включает набор сервисов для обработки больших объемов данных (Big Data): кластер Apache Hadoop [12], распределенную файловую систему HDFS и т.п.
Рис. 2. Схема облачной платформы для вычисления на суперкомпьютерах
Важным аспектом создания облачной платформы является то, что она строится на основе уже имеющихся в СКЦ ИММ УрО РАН серверов. Эти серверы ранее входили в состав вычислительных кластеров, но сейчас уже не обладают необходимой вычислительной мощностью для проведения высокопроизводительных вычислений. Однако эти серверы исправны, пригодны к эксплуатации и могут использоваться для задач, где не требуется высокая вычислительная мощность, в том числе для облачных вычислений. Таким образом, облачную платформу удалось создать без дополнительных затрат на оборудование.
В дальнейшем предполагается увеличить количество серверов облачной платформы и установить дополнительные пакеты прикладных программ, интегрированные с СК.
ОБРАЗОВАТЕЛЬАНАЯ ДЕЯТЕЛЬНОСТЬ
Подготовка специалистов по параллельным вычислениям и суперкомпьютерным технологиям ведется на базовой кафедре ИММ УрО РАН в Институте математики и компьютерных наук УрФУ «Высокопроизводительные компьютерные технологии». В настоящее время кафедра занимается подготовкой магистров направления «Математика и компьютерные науки», специализации «Параллельные вычисления» и «Системное программирование».
Преподавателями кафедры являются сотрудники ИММ УрО РАН, имеющие большой практический опыт работы с технологиями параллельных вычислений, а также преподавательской деятельности.
Практические занятия проводятся с использованием вычислительных ресурсов СКЦ ИММ УрО РАН, в том числе СК «УРАН». Домашние задания студенты также выполняют на СК «УРАН» через Интернет. Таким образом, в процессе обучения студенты получают опыт работы с самой современной техникой для вы-
сокопроизводительных вычислений.
В учебном процессе используются ресурсы вычислительной облачной платформы. В качестве примера можно привести учебный курс «Параллельные вычисления в MATLAB», практические задания которого выполняются с использованием MATLAB из облачной платформы, интегрированным с СК «УРАН».
В целях повышения качества образования и получения самых современных учебных материалов ведется работа по выстраиванию взаимодействия с транснациональными корпорациями, производящими суперкомпьютерное оборудование. Запланировано создание совместно с компанией NVIDIA CUDA Teaching Center [13], который будет проводить учебные курсы по программированию на GPU, а также помогать пользователям СК «УРАН» адаптировать их программы к архитектуре GPU. Совместно с компанией Intel запланировано создание Intel Parallel Computing Center [16], в рамках которого будут проводиться научные исследования в области параллельных вычислений с использованием CPU производства Intel, а также новых ускорителей Intel Xeon Phi.
В подготовке специалистов участвуют и крупные Российские компании. Например, совместно со Школой анализа данных Яндекс [11] на кафедре проводится курс «Параллельные и распределенные вычисления», где, кроме классического подхода к параллельным вычислениям, рассматриваются вопросы распределенной обработки больших объемов данных (Big Data).
ЗАКЛЮЧЕНИЕ
СКЦ ИММ УрО РАН предоставляет научной общественности Уральского региона вычислительные ресурсы мирового уровня, каковым является СК «УРАН». Ведутся работы по созданию распределенной ГРИД-инфраструктуры УрО РАН, в рамках которых создан первый РВЦ в ИМСС УрО РАН, г. Пермь. Для повыше-
ния эффективности и удобства работы пользователей с СК создана вычислительная облачная платформа, в которой по модели SaaS предоставляются пакеты
прикладных программ, интегрированные с СК. Подготовка специалистов по параллельным вычислениям организована совместно с УрФУ.
Библиографический список
1.	Гольдштейн М.Л., Созыкин А.В. Концептуальная модель и прототип ГРИД УрО РАН // Системы управления и информационные технологии. - 2012. - № 3.1 (49). - С. 132-136.
2.	Гольдштейн М.Л., Созыкин А.В., Усталов Д.А. Вычислительная облачная платформа УрО РАН // Научный сервис в сети Интернет: все грани параллелизма: тр. Междунар. суперкомпьютерной конф. (23-28 сентября 2013 г., г. Новороссийск). - М.: изд-во МГУ, 2013. - С. 79-81.
3.	Грид ННС [Электронный ресурс]. URL: http://ngrid.ru (Дата обращения 10.11.2013).
4.	Инициатива GIGA UrB RAS: методология построения и архитектура научно-образовательной оптической магистрали Уральского отделения РАН / А.Г. Масич, Г.Ф. Масич, В.П. Матвеенко, Г.Г. Тирон // Междунар. конф. «Математические и информационные технологии, MIT-2011». -Белград, 2012. - С. 257-265.
5.	Кластер «УРАН» // Параллельные вычисления в УрО РАН [Электронный ресурс]. URL: http://parallel.uran.ru/node/3 (Дата обращения 10.11.2013).
6.	Параллельные вычисления CUDA [Электронный ресурс]. URL: http://www.nvidia.ru/object/cuda-parallel-computing-ru.html (Дата обращения 10.11.2013).
7.	Применение полевых методов измерений для исследования двухфазных потоков / В.Г. Баталов, И.В. Колесниченко, Р.А. Степанов, А. Н. Сухановский // Вестник Пермского университета. Математика. Механика. Информатика. - 2011. - Вып. 5 (9). - С. 21-25.
8.	РДИГ - Российский ГРИД для интенсивных операций с данными [Электронный ресурс]. URL: http://rus.egee-rdig.ru/ (Дата обращения 10.11.2013).
9.	Степанов Р.А., Масич А.Г., Масич Г.Ф. Инициативный проект «Распределенный PIV» // Научный сервис в сети Интернет: масштабируемость, параллельность, эффективность: тр. Всеросс. суперкомпьютерной конф. - М.: изд-во МГУ, 2009. - С. 360-363.
10.	Суперкомпьютеры и ГРИД-сети // Минкомсвязь России. [Электронный ресурс]. URL: http://minsvyaz.ru/ru/directions/?direction=20 (Дата обращения 10.11.2013).
11.	Школа анализа данных Яндекс [Электронный ресурс]. URL: http://shad.yandex.ru/ (Дата обращения 10.11.2013).
12.	Apache Hadoop [Электронный ресурс]. URL: http://hadoop.apache.org (Дата обращения 10.11.2013).
13.	CUDA Teaching Centers [Электронный ресурс]. URL: https://research.nvidia.com/content/cuda-teaching-centers (Дата обращения 10.11.2013).
14.	dCache [Электронный ресурс]. URL: http://www.dcache.org/ (Дата обращения 10.11.2013).
15.	GPU	Accelerated	Applications	[Электронный	ресурс].	URL: http://www.nvidia.com/content/tesla/pdf/gpu-accelerated-applications-for-hpc.pdf (Дата обращения 10.11.2013).
16.	Intel® Parallel Computing Centers [Электронный ресурс]. URL: http://software.intel.com/en-us/articles/intel-parallel-computing-centers (Дата обращения 10.11.2013).
17.	OpenACC Directives for Accelerators [Электронный ресурс]. URL: http://www.openacc-standard.org/ (Дата обращения 10.11.2013).
18.	The Linpack Benchmark // TOP 500 Supercomputer Sites. [Электронный ресурс]. URL: http://www.top500.org/project/linpack/ (Дата обращения 10.11.2013).
19.	TOP 500 Supercomputer Sites. [Электронный ресурс]. URL: http://www.top500.org/ (Дата обращения 10.11.2013).
20.	TOP Green 500 List. [Электронный ресурс]. URL: http://www.green500.org/ (Дата обращения 10.11.2013).
21.	Worldwide LHC Computing Grid | WLCG [Электронный ресурс]. URL: http://wlcg.web.cern.ch (Дата обращения 10.11.2013).
SUPERCOMPUTER CENTER OF INSTITUTE OF MATHEMATICS AND MECHANICS UrB RAS
A.V. Sozykin, M L. Goldshtein
The activities of Supercomputer center of Institute of mathematics and mechanics (UrB RAS) are presented. The current computational resources and the directions of their possible development are described. The questions of creating distributed GRID infrastructures of UrB RAS are considered. The approach to building a cloud platform for high performance computations is presented. Activities in the parallel computational and supercomputing technologies education are described.
Keywords: supercomputers, parallel computing, Big Data, GRID, distributed computing.
Сведения об авторах
Созыкин Андрей Владимирович, кандидат технических наук, заведующий сектором, Институт математики и механики им. Н.Н. Красовского УрО РАН (ИММ УрО РАН), 620990, г. Екатеринбург, ул. Софьи Ковалевской, 16; e-mail: [email protected]
Гольдштейн Михаил Людвигович, кандидат технических наук, заведующий отделом, ИММ УрО РАН; e-mail: [email protected]
Материал поступил в редакцию 25.11.2013 г.
