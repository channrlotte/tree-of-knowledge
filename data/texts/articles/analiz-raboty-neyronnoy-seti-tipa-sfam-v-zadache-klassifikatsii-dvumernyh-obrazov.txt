9.	Войт Н. Н. Разработка методов и средств адаптивного управления процессом обучения в автоматизированном проектировании: дис.... канд. техн. наук / Ульяновский государственный технический университет. - Ульяновск, 2009.
10.	Афанасьев А. Н., Войт Н. Н. Разработка компонентно-сервисной платформы обучения: анализ и разработка компонента метода диагностики проектных характеристик обучаемого инженера с помощью диаграмм UML // Вестник УлГТУ . - 2012. - №4 (60). - С. 43-46.
11.	Афанасьев А. Н., Войт Н. Н. Разработка методов нечёткой параметрической адаптивной диагностики обучаемого инженера // Автоматизация процессов управления. - 2009. - №3. -С. 51-56.
Афанасьев Александр Николаевич, доктор технических наук, первый проректор - проректор по дистанционному и дополнительному образованию УлГТУ, профессор кафедры «Вычислительная техника» УлГТУ. Войт Николай Николаевич, кандидат технических наук, доцент кафедры «Вычислительная техника» УлГТУ, заместитель директора по на-учно-исследовательскойработе ИДДО УлГТУ. Гульшин Владимир Александрович, кандидат технический наук, доцент кафедры «Радиотехника» УлГТУ, начальник Центра подготовки и переподготовки персонала и специалистов иноза-казчика АО «Ульяновский механический завод». Бочков Семён Игоревич, магистрант кафедры «Вычислительная техника» УлГТУ, младший научный сотрудник лаборатории НИР ИДДО УлГТУ.
Поступила 15.12.2016 г.
УДК 004.93
А. В. МИХЕЕВ, С. К. КИСЕЛЕВ
АНАЛИЗ РАБОТЫ НЕЙРОННОЙ СЕТИ ТИПА 8РЛМ В ЗАДАЧЕ КЛАССИФИКАЦИИ ДВУМЕРНЫХ ОБРАЗОВ
Рассмотрены основные принципы функционирования и особенности нейронной сети типа 8ЕЛМ. Затронута проблема выбора архитектуры нейронной сети под задачу. Показаны недостатки сети при классификации образов на двумерных изображениях. Результаты экспериментов проанализированы и обобщены. Дано теоретическое обоснование результатов.
Ключевые слова: нейронная сеть, распознавание образов, классификация, нейронная сеть 8БЛМ.
В задачах классификации, когда правило разделение классов является слишком сложным или его трудно установить, у многих разработчиков возникает соблазн использовать нейронные сети (НС). Безусловно, под такими словами, как обучение и обобщение, скрываются их уникальные сильные стороны, - на основании тестовой выборки аппроксимировать функцию классификации (по сути, сеть сама находит разделяющее правило или алгоритм). Но даже если нейронные сети эффективны для какой-либо задачи, то приходится тратить много времени на выбор нужного типа сети и оптимальных параметров.
© Михеев А. В., Киселев С. К., 2017
Часто разработчики сосредотачиваются не на тех свойствах НС и не на тех особенностях своей задачи, которые действительно будут иметь значения. В технической литературе приходится сталкиваться с огромным объёмом информации, различными трактовкам и часто вольным или даже ошибочным описанием их свойств. Особенно это имеет место в сети Интернет. Так, сети ART, или сети адаптивного резонанса, разработанные Гроссбергом в 1976 г. [1], приобрели новую популярность в 1993 г., когда доктором Ка-субо была предложена Simplified fuzzy ARTMAP, или упрощённая нечёткая сеть адаптивного резонанса [2]. В русскоязычной литературе мало сведений об этой сети и её свойствах, что порождает ложные гипотезы, например,
что её обобщающая способность качественно выше многослойного персептрона, и это приводит к инвариантности распознавания относительно перемещений и поворотов. Считается, что сеть будет распознавать смещённые и повёрнутые образы оригиналов, даже если последние были предъявлены только в одном положении. Целью авторов является доказательство того, что сеть не инвариантна к таким искажениям на основе теории и результатов эксперимента. И что для решения задачи классификации двумерных образов лучше всего использовать сверточную нейронную сеть [5]. Будут рассмотрены общие принципы и алгоритм работы сети для более детального понимания её слабых и сильных сторон. Кроме того, будут даны отдельные замечания, как результат опыта работы с сетью SFAM.
Тип сетей ART существенно отличается от многослойного персептрона и вообще сетей с обратным распространением ошибки. Сети ART, которые обучаются с учителем, называют картами (ARTMAP), так как процесс их обучения можно трактовать как формирование карты классов. В процессе обучения у них изменяются не только веса нейронов, но и создаются новые нейроны, если необходимо запомнить информацию, которая не представлена уже имеющимися нейронами. Данные сети создавали для решения дилеммы стабильности-пластичности, которая проявляется в многослойном персептроне. Если он уже обучен и работает, то при появлении новых данных и необходимости дообучения может быть нарушена уже имеющаяся в нём информация. Человеческий разум способен каждый день узнавать что-то новое, и при этом у него не разрушается память о старом. Именно эту задачу решают сети адаптивного резонанса. Изначально их математическая модель была нацелена на сходство с биологической системой и описывалась в дифференциальных уравнениях, что, конечно, вело к большим трудностям в реализации. Системы на основе этих сетей работали крайне медленно и были сложны в разработке и настройке.
Как уже отмечалось выше, на сегодняшний день разработано много модификаций, которые работают значительно быстрее их аналогов, просты в реализации и настройке [2], [3].
Simplified Fuzzy ARTMAP (SFAM) гораздо быстрее, чем FAM, и проще для понимания и моделирования. Сразу отметим, что рассматриваемая модель не претендует на полноту и математическую строгость и является специфической реализацией для решения практических задач, предложенной в [3]. Кроме того, цель авторов
статьи заключается в анализе возможностей сети, а не в детальном рассмотрении модели. Основные идеи и определения в модели	следующие:
1. Упрощённая сеть состоит из двух слоёв нейронов. Структура сети представлена на рис. 1. Однако входные нейроны не выполняют никакой функции, они лишь принимают комплементарный вектор I.
В
a i
а.
К
а
lN
Рис. 1. Структура сети 8РАМ: I - входной вектор сети; Л- нейроны первого слоя; В - нейроны второго слоя; Тм - значения функций активации нейронов второго слоя
Вектор I состоит из двух равных частей: исходного входного вектора а, подаваемого сети для обучения или классификации, и вектора обратного а:
I = [a., ас] = ,..., ам, ,..., а.^],
С Л
где а I — 1 — Щ,
а М - длина исходного входного вектора.
2.	Каждый нейрон второго слоя В связан со всеми нейронами входного слояЛ. Нейроны второго слоя также как в многослойном персептро-не имеют веса, но в данной модели информация не распределяется по весам нейронов. Один нейрон В сети включает в себя вектор весов, который по сути соответствует некому обобщению входных образов. В сети может быть несколько нейронов, отвечающих за один и тот же класс. Это необходимо для того, чтобы хранить достаточное число вариаций образов одного класса, что повышает способности к обобщению. Кроме того, каждый нейрон В имеет метку (имя), чтобы однозначно идентифицировать, к какому классу относится образ.
3.	Важно понимать, что обобщение образов происходит не путём вычисления алгебраического среднего, а путём нечёткой операции «И» (обозначается «А») над комплементарным вектором. Можно интерпретировать этот механизм как «оседание». Значения весов уменьшаются от максимальной величины до некого устойчивого
состояния (значения), близкого к значениям группы образов одного класса. Это устойчивое состояние и будет обобщённым образом. Подобный механизм обеспечивает лучшую сходимость и сохранение образов, чем при движении значений в двух направлениях.
4.	Принцип обучения сети основан на том, чтобы найти нейрон, содержащий наиболее близкий ко входному образу прототип. Поиск осуществляется в два этапа. Сперва оцениваются уровни активации для всех нейронов (см. формулу (1)). Нейрон с наибольшем значением активации называют победителем. Если победитель найден, то он проверяется на уровень резонанса. Если уровень его резонанса превысил порог, то веса этого нейрона корректируются так, чтобы стать ближе к текущему входному образу. В противном случае порог резонанса увеличивается по заранее заданному правилу и ищется следующий победитель.
5.	Если победитель так и не найден, создаётся новый нейрон, веса которого принимаются близкими к текущему входному вектору. Если в=1, то веса нового нейрона полностью совпадают со входным вектором.
6.	Резюмируя всё вышесказанное, отметим, что главная особенность SFAM в том, что сеть расширяется, если не находит в памяти образа, схожего со входным. Если же образ находится, то сеть не расширяется, а веса нейрона, с наиболее близким вектором из имеющихся в её памяти, подтягиваются ко входному вектору.
7.	Упрощения по сравнению с обычной ART заключаются в том, что авторы избавились от избыточности и сделали структуру сети более простой для понимания. Например, последние исследования показали, что сортировка нейронов по уровню активацию не обязательна. В большинстве случаев резонанс наблюдается у первых нейронов, поэтому сортировку можно исключить и лишь выбирать нейрон с максимальным значением активации, а активацию предыдущего нейрона обнулять. Обо всех нововведениях можно узнать в [3].
8.	Прежде чем подать входной вектор на вход сети, значения этого вектора следует нормализовать в диапазоне от 0 до 1.
9.	Неназначенный нейрон - это нейрон, все веса которого равны единице. После назначения нейрона к некому классу, он становится назначенным, и его вектор весов становится равным текущему входному вектору, и эти веса становятся наиболее близким образом для текущего входа.
10.	Назначенный нейрон является представителем какого-то класса, а его вектор весов и есть
обобщение какой-то группы образов этого класса. Уровень активации такого нейрона считается по формуле (2).
11. Принципы работы сети в режиме классификации во многом идентичны обучению, за исключением того, что обновления и создания нейронов не происходит.
Алгоритм обучения достаточно простой (рис. 2) и состоит из следующих шагов:
1.	Назначить стартовый порог резонанса
Р = Рstart.
2.	Подать входной вектор и определить выходной вектор второго слоя. Функция для расчёта активации нейронов второго слоя имеет вид
(2)
Ъ(/)= S fori = 1...N- 1,
а+\мц
где N - общее число нейронов. Пары, номер нейрона и значение его активации, необходимо хранить в списке для каждого входного вектора. 3. Найти победителя по следующей формуле:
(3)
J = arg[ma.Xj(Tj^].
Если победитель не найден, то перейти к шагу 7.
4. Проверить состояние резонанса, то есть достаточно ли входной вектор похож на прототип:
\Mwyl \Mwyl \1\ = М
>
p,
(4)
Если резонанс есть, то перейти к шагу 5.
Если резонанса нет, то перейти к шагу 3 и искать следующего победителя.
5.	Если образ победителя близок к образу входа, то следует обновить веса победителя следующим образом:
^пеы) =	+ (1 _	(5)
и перейти к шагу 8.
В противном случае нужно сбросить победителя (установить уровень его активации Т = 0), временно повысить порог резонанса.
\MWil
р = Чт +(6)
где е - это маленькое положительное число. Рекомендуемое значение е =0,001.
6.	Если порог резонанса больше 1, то остановить процесс обучения для входного образа в текущей эпохе обучения и перейти к шагу 8, в противном случае - к шагу 3 и пытаться искать следующего победителя.
7.	Создать новый нейрон с единичными весами, назначить ему метку класса как у входного вектора, после чего подтянуть веса следующим образом:
Wj
(new)
=
¡Awlold)) + (1 -P)wl0ld),
(7)
при этом значение N увеличивается на единицу.
8. Перейти к шагу 1 и повторить алгоритм для следующего входного вектора.
Алгоритм работы во многом идентичен алгоритму обучения за исключением следующих моментов:
1. Новые нейроны не создаются в случае, если нет назначенных нейронов с высоким уров-
нем резонанса. Ответом сети при этом может быть специальная метка, говорящая о том, что сеть не распознала данный класс.
После того как значение резонанса у нейрона победителя превышает порог, происходит выдача его метки как ответа сети. Обновление его весов не производиться.
Рис. 2. Блок-схема алгоритма обучения 8БЛМ для одного входного вектора и одной эпохи обучения
Замечания к формулам и алгоритмам:
1.	Алгоритм обучения стартует с одного не назначенного нейрона.
2.	Оператор «И» в нечёткой логике определяется как
(рЛ q)( = min (pi, qt).	(8)
3.	Оператор «| |» определяется как:
|P|= Zj=c Pi.	(9)
4.	Если новый неназначенный нейрон выбран как победитель, то поиск победителей заканчивается.
5.	р — это порог резонанса. Допустимый интервал для р [0, 1]. Начальное состояние переменной р определяется для каждого входного шаблона и каждого цикла обучения. Когда новый входной образ I ассоциирован с ошибочным прототипом, фактор резонанса должен быть временно увеличен, чтобы исключить ассоциации с ним, и затем алгоритм сформирует новый прототип на основании входного вектора. Это называется «прямой доступ к свойствам» и означает, что первый прототип, который получит победителя, гарантированно будет ближе к нужному классу.
6.	При увеличении р этого значения число создаваемых классов и их прототипов также увеличивается. Большое число классов даст мелкозернистую классификацию. Это иногда улучшает качество распознавание, если в данных много шума или иных данных. С другой стороны, большое число классов увеличивает время обучения и снижает производительность сети при работе.
7.	а - это параметр выбора (0,001 <а <10). Увеличение переменной а приводит к увеличению количества классов, которые создаёт сеть. В начале целесообразно ставить значение 0,001.
8.	ß - это коэффициент обучения (0 < ß <=1). SFAM может обучаться в двух режимах: быстрое обучение и медленное. При быстром обучении ß = 1. При медленном ß = 1, если нейрон не определён, в противном случае 0 < ß < 1. В медленном режиме будет создана больше прототипов. Этот режим эффективен для зашумлён-ных данных. Но иногда медленный алгоритм обучения избавляет от необходимости предоставления всей обучающей выборки, особенно когда а или 8 имеют большие значения.
9.	При нормировке входного вектора можно использовать различные методы, сохраняющие информацию по амплитуде. Если такая информация будет искажена, то число классов может стремительно расти, не приводя к корректному обучению.
10.	Перед тестом сети на тестовой выборке все созданные прототипы, которые ни разу не обозначались как победители являются ложными и должны быть удалены.
11.	Порядок следования данных может оказать воздействие на производительность сети. То есть для различного следования данных число создаваемых классов будет меняться, и, как следствие, это скажется на производительности. При использовании стратегии голосования сеть тренируется некоторое время на различных перестановках одних и тех же тренировочных данных. Поскольку раз за разом ответ сети может меняться, все ответы регистрируются, и в итоге входной образ относится к тому классу, который заработал наибольшее число голосов.
Теперь рассмотрим возможности сети в контексте задачи распознавания двумерных образов. В качестве реализации была использована математическая модель, созданная в Ма1;ЬаЬ 2013а с помощью стандартных пакетов моделирования.
Обучение сети на конкретных данных — процесс очень специфичный и вызывает много трудностей с подготовкой данных, формированием выборок, настройкой различных коэффициентов и собственно тестированием сети. Нужно понимать, что если у разработчиков нет опыта отладки НС и задача далеко не типовая, то им придётся отклониться от первичной задачи и решать уже задачу настройки сети, создание или изучение ряда дополнительных инструментов. Много инструментов для прототипирования НС есть в пакете Ма1;ЬаЬ, но их рассмотрение выходит за рамки данной статьи.
Для генерации данных была разработана программа на языке С#. С её помощью можно было на основе чёрно-белого двумерного изображения в формате Ьтр сгенерировать серию изображений, отличающихся от оригинала. В качестве отличий можно было задавать различные параметры. Уровень шума определялся в процентах от значащих точек, так если цвет пикселя соответствует цвету фона, такой пиксель считает незначащей точкой. Сила размытия определялась в пикселях, так, сила в 1 пиксель означает, что каждый соседний пиксель к значащему (к значащей точке) будет закрашен. Диапазон сдвигов определялся максимальным сдвигом в пикселях по оси, причём программа генерировала случайный сдвиг из диапазона для всех значащих точек для каждого нового образа. Диапазон разброса определялся максимальным числом пикселей, на которое могла сдвинуться значащая точка по оси. Для каждого нового генерируемого образа эти точки выбирались из оригинала случайным образом и сдвигались на случайное значение
(а)	(б)	(в)	(г)
Рис. 3. Примеры изображений с образами, которые использовались для генерации данных,
(а)	- идеальный образ представляет собой два горизонтальных отрезка, расположенных параллельно друг другу и находящихся в верхней части изображения;
(б)	- идеальный образ представляет собой два горизонтальных отрезка, расположенных параллельно друг другу и находящихся в середине изображения. Размеры этих отрезков в большинстве случаев должны быть больше образа (а). Расстояние между отрезками в обоих образах должно быть не больше 10 пикселей;
(в)	- идеальный образ представляет собой пунктирную дугу из шести штрихов в верхней части изображения;
(г)	- идеальный образ представляет собой два отрезка, сходящихся под острым углом (пересечение не является обязательным). Расположение в верхней части изображения, острый угол также направлен вверх изображения
(а)	(б)	(в)	(г)
Рис. 4. Примеры изображений с образами, которые получились путём генерации данных,
(а)	- демонстрирует небольшой разброс пикселей и размытие;
(б)	- демонстрирует добавление шума и разброс пикселей;
(в)	- демонстрирует разброс пикселей и сдвиг образа;
(г)	- демонстрирует сдвиг, разброс и зашумление пикселей
из этого диапазона. Общее число сдвинутых точек определялось процентом разброса, который определялся относительно значащих точек на оригинальном изображении. После всех преобразований данные можно было сохранить как изображение или как числовой вектор для использования в пакете Ма1;ЬаЬ.
Примеры оригинальных образов представлены на рис. 3.
Как видно из примеров, важную роль в образах имеют не абсолютные значения векторов (значения амплитуды у каждого элемента вектора а = 1), а форма линий (узоров). Примеры данных, сгенерированных на основе оригиналов, представлены на рис. 4.
Был проведён ряд тестов на различных выборках при различных настройках сети. Важно отметить, что система не настроена на максимальную скорость работы. В большинстве результатов скорость работы и вовсе не учитывается, так как для выводов важны лишь относительные изменения времени работы от количества нейронов сети и данных. Кроме того, не было цели достигнуть максимально возможного качества распознавания. Размеры всех изображений во всех тестах были таковы: 75 пикселей в ширину и 200 пикселей в высоту.
Проверка работоспособности сети проводилась на обучающей выборке из четырёх классов по 10 экземпляров для каждого. Тестовая выборка была идентичной, и сеть показала 100% распознавания, создав 40 нейронов при в = 1 и р = 1,0. Также были определены оптимальные параметры, при которых сеть показала 100% распознавания, создав 9 нейронов при в = 0,75 и р = 0,95.
Тест №1 проводился на обучающей выборке из двух классов по 100 образов в каждом классе. Тестовая выборка генерировалась из тех же оригиналов, что и обучающая выборка, и её размер также составил 200 экземпляров. Во всех случаях р = 0,975, обучение проводилось в одну эпоху. Детальные параметры приведены в таблице 1, а результаты в таблице 2.
По результатам теста видно, что если образы тестовой выборки имеют незначительные отличия, то удаётся найти оптимальные параметры и получить высокий процент распознавания при хорошем уровне обобщения. Кроме того, тестовая выборка генерировалась на основе обучающей. В реальных задачах отличия образов будут значительно выше, следовательно, сеть покажет плохой результат.
Тест №2 проводился на обучающей выборке из двух классов по 200 образов в каждом классе. Тестовая выборка генерировалась не из тех же
Параметры теста №1
Наименование параметра	Обучающая выборка	Тестовая выборка
Сила размытия, пкс.	0	0
Процент шума, %	0	15
Диапазон сдвигов по X, пкс.	20	25
Диапазон сдвигов по У, пкс.	10	15
Диапазон разброса по Х, пкс.	0	0
Процент разброса по Х, %	0	0
Диапазон разброса по У, пкс.	0	0
Процент разброса по У, %	0	0
Таблица 2
Результаты теста №1
Показатель	в =0.5	в =0.75	в =0.9	в =0.95
Процент распознавания	80.0	88.5	95.0	89.5
Число созданных нейронов	25	21	24	23
Таблица 3
Параметры теста №2
Наименование параметра	Обучающая выборка	Тестовая выборка
1 часть	2 часть
Сила размытия, пкс.	0	0	0
Процент шума, %	0	5	10
Диапазон сдвигов по X, пкс.	20	20	20
Диапазон сдвигов по У, пкс.	10	10	10
Диапазон разброса по Х, пкс.	0	1	3
Процент разброса по Х, %	0	10	5
Диапазон разброса по У, пкс.	0	1	3
Процент разброса по У, %	0	10	5
Таблица Результаты теста №2
Показатель	в =0.5	в =0.75	в =0.8	в =0.9	в =0.95
Процент распознавания	65.0	79.1	84.2	75.0	65.5
Число созданных нейронов	25	21	24	24	23
оригиналов, что и обучающая выборка. Во всех случаях р = 0,975, обучение проводилось в одну эпоху. Детальные параметры приведены в таблице 3, а результаты в таблице 4.
По результатам теста видно, что если образы тестовой выборки имеют такой же диапазон смещений, то удаётся найти оптимальные параметры и получить высокий процент распознавания при хорошем уровне обобщения. При этом тестовая выборка была сгенерирована из других оригиналов, а искажения существенно выше, чем в тесте №1. Это говорит о том, что сеть эффективно справляется с искажениями, не связанными с пространственной трансформацией.
Тест №3 проводился на обучающей выборке из четырёх классов по 250 образов в каждом классе. При этом первые результаты, приведённые в таблице 6, получены при обучения только
первой частью выборки (то есть без размытия). Результаты, приведённые в таблице 7, получены при обучении двумя частями выборки. Тестовая выборка генерировалась не из тех же оригиналов, что и обучающая выборка, и на каждый класс пришлось по 250 экземпляров. Обучение проводилось в одну эпоху. Детальные параметры приведены в таблице 5.
По результатам теста видно, что при наличии четырёх классов образов, в которых решающую роль играет форма, качество распознавания заметно снижается. Однако при добавлении размытия характеристики улучшаются. Это происходит из-за того, что размытие позволяет образам занимать большую площадь.
Тест №4 проводился на обучающей выборке из четырёх классов, по 250 образов в каждом классе. Тестовая выборка генерировалась не из
Параметры теста №3
Наименование параметра	Обучающая выборка	Тестовая выборка
1 часть	2 часть	1 часть	2 часть
Сила размытия, пкс.	0	1	0	0
Процент шума, %	0	0	15	25
Диапазон сдвигов по X, пкс.	30	30	20	30
Диапазон сдвигов по У, пкс.	10	10	10	10
Диапазон разброса по Х, пкс.	1	2	2	3
Процент разброса по Х, %	20	5	20	10
Диапазон разброса по У, пкс.	1	2	2	3
Процент разброса по У, %	20	5	20	10
Таблица 6
Результаты теста №3
Показатель	в =0.5	в =0.75	в =0.9
р = 0,95	р = 0,95	р = 0,97	р = 0,99	р = 0,97
Процент распознавания	17.8	16.7	47.2	6.4	18.6
Число созданных нейронов	191	164	214	457	245
Таблица 7
Результаты теста №3
Показатель	в =0.5	в =0.75	в =0.9
р = 0,95	р = 0,95	р = 0,97	р = 0,99	р = 0,97
Процент распознавания	25.8	22.3	52.9	10.2	24.0
Число созданных нейронов	167	152	232	410	209
Таблица 8
Параметры теста №4
Наименование параметра	Обучающая выборка	Тестовая выборка
1 часть	2 часть	1 часть	2 часть
Сила размытия, пкс.	0	1	0	0
Процент шума, %	0	0	20	20
Диапазон сдвигов по X, пкс.	30	30	45	45
Диапазон сдвигов по У, пкс.	10	10	15	15
Диапазон разброса по Х, пкс.	1	2	2	3
Процент разброса по Х, %	20	5	20	10
Диапазон разброса по У, пкс.	1	2	2	3
Процент разброса по У, %	20	5	20	10
Таблица 9
Результаты теста №4
Показатель	в =0.5	в =0.75	в =0.9
р = 0,95	р = 0,95	р = 0,97	р = 0,99	р = 0,97
Процент распознавания	23.4	18.9	49.9	4.3	18.7
Число созданных нейронов	191	164	339	457	245
Время работы, с	175	144	393	584	241
тех же оригиналов, что и обучающая выборка. Обучение проводилось в две эпохи. Тестовая выборка составила 1800 экземпляров. Детальные параметры приведены в таблице 8, а результаты в таблице 9.
По результатам теста видно, что при наличии четырёх классов образов, в которых решающую
роль играет форма, и пространственные сдвиги в тесте были больше, чем в обучении, качество распознавания заметно снижается, и даже при добавлении размытия характеристики не становятся намного лучше. Этот эксперимент показывает, что без обобщения пространственных признаков сеть может хранить множество вариантов
размещения образов, что, однако, ведёт к разрастанию числа нейронов и гигантским требованиям к памяти (существенно большим, чем у свёрточной сети).
Однако, кроме неверного распознавания предоставленного образа, в реальных задачах существует ещё один тип ошибки классификационной системы, когда образ распознается там, где вообще нет искомых образов. В таком случае на полных данных размытие может только усугубить результаты, так как большие площади образов в памяти будут перекрывать случайные сигналы, шумы и т. п.
Выводы:
Очевидно, что топология образов не учитывается НС типа 8БЛМ. Это происходит потому, что ни функция активации (2), ни функция резонанса (6), которые являются своеобразными метриками расстояния, не учитывают пространственные связи элементов.
Если векторы имеют небольшую размерность, то повышая порог резонанса, можно добиться того, что все варианты пространственных изменений попадут в память сети. Но такое решение явно не является адекватным выбранному инструменту. Увеличение числа нейронов будет приводить к нелинейному увеличению времени работы и большому объёму требуемой памяти. Даже при уменьшении размеров изображения под каждый класс потребуются тысячи нейронов для полного сохранения информации.
Плюсы:
1.	Плавное подтягивание сохранённого вектора к обучаемому множеству, что обеспечивает стабильность в обучении и лучшие характеристики обобщения.
2.	Способность хранить несколько прототипов образа одного и того же класса ещё на порядок повышает обобщающие способности сети. Подобных свойств нет у классических методов и у многослойного персептрона.
3.	Информация, представленная в сети, достаточно просто интерпретируется в отличие от многослойных персептронов.
4.	Отсутствие необходимости опытным путём подбирать в каждом конкретном случае соответствующее число скрытых слоёв и нейронов в каждом из них.
5.	Число итераций для обучения существенно меньше, чем для многослойных сетей [3].
6.	Алгоритм сети достаточно эффективно трансформируется для параллельных систем.
Минусы:
1.	Сеть не инвариативна к пространственным искажением образа.
2.	Наличие большой обучающей выборки для обобщения.
3.	Данный тип сети может применяться только для задач классификации.
4.	Далеко не во всех случаях SFAM работает быстрее и даёт лучшие результаты, чем многослойный персептрон [3].
Поэтому, для того чтобы использовать все плюсы сетей SFAM, необходимо проводить дополнительную обработку данных и извлекать из двумерных образов различные геометрические признаки сторонними методами.
Одним из таких сторонних методов могут служить статистические моменты. Результатом их вычисления будет вектор инвариантных оценок, описывающих исходный образ. Статистические моменты по своей сути описывают двумерные распределения некоторых величин с точки зрения вероятности. Однако двумерное изображение легко представляется как двумерное распределение, о чём подробно написано в [4]. Полученный геометрически инвариантный вектор статистических моментов подаётся на вход сети SFAM. То есть классификатор обучается и работает уже на другом уровне данных. В этом случае качество распознавания увеличиться, хотя и не превзойдёт решения на основе свёрточных нейронных сетей. Кроме того, при дополнительном уровне извлечении признаков информация в сети потеряет наглядность, столь необходимую при отладке. Поэтому для решения задач классификации образов, в которых важны пространственные признаки, лучшим инструментом будут свёрточные нейронные сети. Однако это не значит, что сеть типа SFAM не является плохим инструментом. Эта архитектура лучше подойдёт для задач, где данные представляют собой одномерные векторы, мало обучающих данных, классы будут добавляться в будущем и крайне важна интерпретируемость данных в сети. Так эта сеть успешно применяется в медицине, для анализа электроэнцефалограмм (ЭЭГ) и ЭХОЭЭГ.
СПИСОК ЛИТЕРАТУРЫ
1. Carpenter G. A., Grossberg S., Markuzon N. and Rosen D. B. FuzzyARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps / G. A.Carpenter, S. Grossberg, N. Markuzon, D. B. Rosen // IEEE Trans. Neural Networks. 1992. №3 (5). p. 698-713.
2.	Kasuba T. Simplified fuzzy ARTMAP / T. Kasuba // AI Expert.-Nov. 1993. p. 18-25.
3.	Mohammad-Taghi Vakil-Baghmisheh, Nikola Pavesic. A Fast Simplified Fuzzy ARTMAP Network /Mohammad-Taghi Vakil-Baghmisheh, Nikola Pavesic // Klawer Academic Publishers. Neural Processing Letters. 2003. №17. p. 273-316.
4.	Rajasekaran S., Vijayalakshmi Pai, G. A. Simplified Fuzzy ARTMAP as Pattern Recognizer / S. Rajasekaran, G. A. Vijayalakshmi Pai // Journal of computing in civil engineering. April 2000. p. 92-99.
5.	Krizhevsky A., Sutskever I., Hinton G. E. Imagenet classification with deep convolutional neural networks / A. Krizhevsky, I. Sutskever, G. E.
Hinton //Advances in neural information processing systems. 2012. p. 1097-1105.
Михеев Александр Вячеславович, аспирант кафедры ИВК ФИСТ УлГТУ.
Киселев Сергей Константинович, доктор технических наук, заведующий кафедрой «Измерительно-вычислительные комплексы» ФИСТ УлГТУ.
Поступила 08.02.2017 г.
УДК 004:378
В. Г. ТРОНИН, С. В. СКВОРЦОВ
АВТОМАТИЗАЦИЯ РАСЧЁТА РЕЙТИНГА НАУЧНОЙ АКТИВНОСТИ
Рассмотрена методика расчёта рейтинга научной активности преподавателей вуза. Выявлены возможные источники данных и трудности при автоматизации. Сделан вывод о потребности автоматизации цепочки процессов с применением данных не только для расчёта рейтинга, но и для различных отчётов, создания базы по учёным университета с автоматизацией сбора информации для подготовки заявок на конкурсы. Рассмотрены особенности автоматизации учёта потока публикаций университета.
Ключевые слова: рейтинг, наукометрия, РИНЦ, 1С, автоматизация процессов университета.
Введение
Ежегодно в университете проводится расчёт рейтинга научной активности сотрудников (преподавателей и научных работников), кафедр, факультетов. По итогам года определяются учёные, внесшие наибольший вклад в достижения вуза, организуется их поощрение. При расчёте рейтинга анализируется разнородная информация, значительная часть которой собирается вручную. Часть информации соответствует показателям вуза, по которым выполняет оценку Минобрнауки РФ, часть нужна для организации научного процесса. Сама информация в процессе подсчёта пополняется (выходят в свет журналы, проводятся конференции в последние месяцы года, выявляются неточности при подготовке отчётных данных), что вызывает вопросы при окончательной оценке сотрудника.
© Тронин В. Г., Скворцов С. В., 2017
Что учитывается в рейтинге?
Рейтинг рассчитывается для каждого сотрудника по 97-ми позициям анкеты (в 2016 г., изначально было 20 позиций), каждая из которых оценивается в баллах. При этом количество позиций оценки ежегодно изменяется в соответствии с необходимостью акцентировать работу на соответствующем направлении как сотрудника, так и всего университета. Баллы за виды активности установлены в предположении, что высшим достижением является защита докторской диссертации, остальные виды научной деятельности в шкале баллов сопоставлены с данным достижением. Каждый участник рейтинга заполняет в Excel таблицу по шаблону и в Word пишет расшифровку по заполненным позициям. Рейтинг учитывает именно те показатели, которые важны для выполнения аккредитационных и мониторинговых показателей, устанавливаемых
