Труды ИСП РАН, том 27, вып. 6, 2015 г.
Облачный сервис для решения многомасштабных задач нанотехнологии на суперкомпьютерных системах
С. В. Поляков <[email protected]>
А.В. Выродов <[email protected]>
Д.В. Пузырьков <[email protected]>
М.В. Якобовский <lira@,imamod.ru>
ИПМ им.М.В.Келдыша РАН,
125047, Россия, г. Москва, Миусская пл. 4
Аннотация. В работе представлены структура и отдельные компоненты облачного сервиса, предназначенного для решения многомасштабных задач нанотехнологии на суперкомпьютерных системах. Мотивацией к созданию именно облачного сервиса была необходимость интеграции идей и знаний по данной прикладной проблеме, специалистов по решению задач данного класса на суперкомпьютерных системах, различных технологий моделирования и множества пакетов прикладных программ, а также различных вычислительных ресурсов, имеющихся у ИПМ и его партнеров. Итогом работы стал прототип облачной среды, реализованный в виде сервиса Мультилогин и прикладного программного обеспечения доступного из виртуальных машин пользователей. Первым приложением сервиса стала параллельная программа Flow_and_Particles для суперкомпьютерных расчетов многомасштабных задач газовой динамики в микроканалах сложных технических систем и визуализатор результатов расчетов Flow_and_Particles_View.
Ключевые слова: облачные сервисы и технологии; многомасштабные задачи газовой динамики; суперкомпьютерное моделирование.
1.	Введение
Настоящая работа посвящена развитию распределенных облачных вычислений при решении задач нанотехнологий. Конкретная задача состоит в создании облачного сервиса для многомасштабного моделирования нелинейных процессов в полидисперсных многокомпонентных средах с помощью гетерогенных кластеров и суперкомпьютеров. Фундаментальность и актуальность общей проблемы состоит в том, что в настоящее время в связи с внедрением нанотехнологий во многих отраслях промышленности существует острая необходимость объединения различных математических подходов, информационных и вычислительных	ресурсов в единый аппарат
суперкомпьютерного	моделирования.	Наиболее	удачным способом
409
Trudy ISP RAN [The Proceedings of ISP RAS], vol. 27, issue 6, 2015.
объединения является создание соответствующих облачных сред и сервисов, в которых каждый пользователь сможет иметь доступ ко всем возможным информационным материалам, моделирующим программам, вычислительным
ресурсам и индустриальным САПР.
В рамках настоящей работы предлагалось создать прототип облачного сервиса на базе кластеров ИПМ им. М.В. Келдыша РАН, ориентированного на суперкомпьютерное моделирование задач нанотехнологий методами механики сплошной среды и молекулярной динамики. Специфика выбранного направления научных исследований связана с распределенной параллельной обработкой на кластерах и суперкомпьютерах больших объемов данных, связанных с моделируемой средой как на макроуровне, так и в микромире. Примером конкретной задачи в выбранной прикладной области может служить моделирование многомасштабных нелинейных процессов взаимодействия газовых сред со стенками металлических каналов в технических микросистемах [1-3], используемых в нанотехнологиях. Специфика подобного рода задач состоит в проведении множества детальных вычислительных экспериментов, касающихся определения как свойств отдельных веществ (металлов и газовых сред), так и свойств результирующих многокомпонентных и многофазных течений газов в рассматриваемых реальных технических системах. Проведение указанных вычислительных экспериментов невозможно без использования самой современной высокопроизводительной компьютерной и суперкомпьютерной техники. Использование последней без создания соответствующих интегрирующих сред и сервисов (каковыми и являются облачные среды и сервисы) существенно снижает эффективность параллельных вычислений, реализованных в прикладном программном обеспечении ПО.
Конкретными задачами настоящей работы были:
•	организация облачной среды и виртуального пространства пользователей;
•	реализация удобного и эффективного интерфейса пользователей с доступными им различными удаленными компьютерными и суперкомпьютерными системами;
•	разработка и верификация прикладного программного обеспечения, предназначенного для решения многомасштабных задач нанотехнологии на примере расчета течений газовых смесей в металлических микроканалах;
•	разработка системы управления и мониторинга расчетными заданиями пользователей на удаленных вычислительных системах;
•	разработка системы хранения, пред- и пост- обработки, а также визуализации больших массивов распределенных данных, связанных с проведением вычислительных экспериментов на удаленных кластерах и суперкомпьютерах.
410
Труды ИСП РАН, том 27, вып. 6, 2015 г..
Для решения поставленных задач и достижения общих целей работы к настоящему моменту получены результаты, обсуждаемые в последующих пунктах.
2.	Система KIAM Multilogin
В рамках работы разработана система KIAM Multilogin, которая является облачным VDI сервисом (удаленный рабочий стол), предоставляющим доступ пользователям к персональным виртуальным машинам. Доступ для пользователей возможен как из Интранет, так и из Интернет сетей по открытым (HTTP) и шифрованным (HTTPS, SSH, VPN) протоколам. Виртуальные машины управляются двумя основными типами ОС: Linux и Windows. Виртуальные машины используются для:
•	работы с прикладными пакетами, системами моделирования и научными базами данных;
•	создания и тестирования собственных приложений;
•	запуска их на счет на доступных вычислительных ресурсах (внутренних и внешних);
•	мониторинга за запущенными приложениями;
•	обработки, анализа и визуализации результатов расчётов.
Вся система KIAM Multilogin построена исключительно на программных компонентах с открытым кодом таких как: Centos Linux, Ovirt, OpenStack, Ceph, Apache Directory Server, GlusterFS и пр. Система Мультилогин построена с учетом современных тенденций SDDC (программноопределяемый дата-центр). Все программные компоненты установлены на сервера х86-х64 архитектуры. В серверах установлены стандартные жесткие диски. В качестве ЛВС используются простые коммутаторы Ethernet 1 Гбит/с. Архитектура системы KIAM Multilogin показана на рис. 1. Она состоит из четырех основных элементов:	подсистемы удаленного доступа и
маршрутизации, подсистемы виртуализации, хранения виртуальных машин и доступа к их рабочим столам, подсистемы аутентификации и авторизации, подсистемы хранения пользовательских данных.
Инфраструктура удаленного доступа и маршрутизации показана на рис. 2. В настоящий момент она обслуживается одним сервером (imm5), однако в будущем предполагает использование нескольких дублирующих серверов с целью повышения отказоустойчивости. Инфраструктура виртуализации, хранения виртуальных машин и доступа к рабочим столам виртуальных машин в целом показана на рис. 3. Ее подсистема хранения пользовательских данных показана на рис. 4. Последняя реализована посредством 10 серверов с суммарным объемом дисковой памяти 50 Тб и полезным пространством 20 Тб.
411
Trudy ISP RAN [The Proceedings of ISP RAS], vol. 27, issue 6, 2015.
Инфраструктура удаленного доступа и маршрутизации
I
Инфраструктура виртуализации, хранения виртуальных машин и доступа к рабочим стопам виртуальных машин VDi
I
1
Аутенти фикация и авторизация
I
Инфраструктура хранения пользовательских данных
Рис. 1. Архитектура системы Multilogin.
Рис. 2. Инфраструктура удаленного доступа и маршрутизации.
412
Труды ИСП РАН, том 27, вып. 6, 2015 г..
Рис. 3. Инфраструктура виртуализации, хранения виртуальных машин и доступа к рабочим столам виртуальных машин VDI.
Пол ьзовдтели
smb/clfs
smb/cifs
ctphfc
clustered samba service /cephfs/home
ceph storage
imml4
imml5
Immld
Puc. 4. Инфраструктура хранения пользовательских данных.
413
Trudy ISP RAN [The Proceedings of ISP RAS], vol. 27, issue 6, 2015.
Система KIAM Multilogin позволяет быстро и своевременно добиваться требуемых результатов благодаря, в том числе:
•	высокому уровню доступности;
•	отказоустойчивости от единичных сбоев оборудования;
•	эффективному использованию вычислительных ресурсов;
•	минимальному времени простоя при регламентных работах.
Система KIAM Multilogin отличается от обычных распределенных Web-сервисов аппаратно-программной независимостью сессий пользователей и повышенной отказоустойчивостью. При этом допускается длительный период жизни одной сессии, подключение к ней с разных компьютеров, а также миграция активной сессии с одного физического сервера на другой.
3.	Система KIAM JobjControl
Также в рамках работы создана система управления и мониторинга расчетными заданиями пользователей KIAM JobControl, выполняющимися на различных кластерах и суперкомпьютерах, расположенных как в локальной сети ИПМ им. М.В.Келдыша РАН, так и за ее пределами. Основной задачей системы является эффективное управление расчетами пользователя в ситуации, когда объем контрольной точки составляет несколько гигабайт и более, а результирующие данные могут превосходить терабайт.
Стратегия системы состоит в том, чтобы спрогнозировать очередной запуск задания на основе данных о его положении в системах очередей доступных кластеров и суперкомпьютеров и обеспечить к моменту запуска наличие актуальной расчетной точки на данном вычислителе. После проведения кванта расчетов система должна обеспечить в фоновом режиме перекачку промежуточных или окончательных результатов расчетов на кластер хранения и обработки данных расчета.
От запущенного параллельного приложения требуется максимальная компактность контрольной точки и результатов расчетов, информация о минимальном размере кванта расчета на соответствующей параллельной конфигурации и времени сохранения данных (контрольной точки и результатов), а также маски сохраняемых файлов. Данная информация может быть записана приложением в специальный файл или введена пользователем при настройке интерфейса мониторинга.
Обращение к системе KIAM Job Control возможно как напрямую с компьютера пользователя (находящегося либо в локальной сети Института или во внешней сети) в режиме "limited access", так и из виртуальной машины в режиме "full access". Таким образом, полный функционал осуществляется только через сервис KIAM Multilogin.
414
Труды ИСП РАН, том 27, вып. 6, 2015 г..
4.	Приложение Flow_and_Particles
В качестве первого приложения создаваемого облачного сервиса выбрана задача многомасштабного моделирования течений газов в микроканалах технических систем в условиях многих масштабов расчетной области. В частности, рассмотрена задача о течении азота в никелевом микроканале. Основное внимание в этой задаче уделяется расчету макропараметров газовой среды. Различие в масштабах расчетной области (длина канала, поперечное сечение канала, длина свободного пробега молекул, толщина пограничного слоя) и приповерхностное взаимодействие газа с металлом приводят к необходимости учитывать рельеф и свойства микроканала на молекулярном уровне. В результате математическая модель исследуемого течения не может быть полностью сформулирована в рамках макроскопического подхода.
При реализации математической модели используется мультимасштабный подход, сочетающий решение уравнений квазигазодинамики (КГД) и коррекцию газодинамических параметров методом молекулярной динамики (МД). Общий алгоритм представляет собой расщепление по физическим процессам. КГД система уравнений решается методом конечных объемов. Система уравнений МД используется в качестве подсеточного алгоритма, применяющегося внутри каждого контрольного объема, и решается с помощью схемы Верле. В МД-вычислениях взаимодействие частиц описывается с помощью потенциалов, определяющих основные свойства компонент газовой смеси. Подробно этот подход освящен в работах [1-3]. Параллельная реализация подхода основана на методах расщепления по физическим процессам и разделения областей. Компьютерная реализация выполнена в виде приложения Flow and Particles, входящего в программный комплекс GIMMNANO [4] (разработан в рамках госконтракта № 07.524.12.4019 Минобрнауки РФ), и ориентирована на использование вычислительных систем с центральной и гибридной архитектурами. При ее создании использовались концепция гибридной параллельной вычислительной платформы [5] и такие технологии параллельного программирования как MPI, ОрепМР и CUDA.
Тестирование разработанного приложения Flow_and_Particles проводилось на суперкомпьютерах К100 (ИПМ им. М.В. Келдыша РАН), МВС-10П (МСЦ РАН), кластер с сетью Ангара (АО "НИЦЭВТ"). Предварительные расчеты показали, что общий численный алгоритм устойчив к использованию корректирующих течение данных, полученных в результате МД-вычислений. С его помощью методами МД были получены основные коэффициентные зависимости для КГД-системы, проверен переход от МД к КГД и обратно, произведен расчет плоского и полностью трехмерного течений в микроканалах с диаметрами от 10 до 30 мкм и длиной от 60 до 120 мкм. Полученные результаты подтвердили эффективность разработанного подхода.
415
Trudy ISP RAN [The Proceedings of ISP RAS], vol. 27, issue 6, 2015.
5.	Программа визуализации Flow_and_Particles_View
Для просмотра результатов расчетов программы Flow and Particles разработана программа распределенной визуализации FlowandParticlesView, предназначенного для сбора, обработки и визуализации распределенных результатов моделирования, расположенных на удаленном вычислительном кластере и хранящихся частями в различных директориях сетевой файловой системы. Реализация программного комплекса выполнена на языке программирования Python с использованием известных пакетов для анализа данных SciPy [6] и средства визуализации Mayavi [7]. В качестве системы управления используются скрипты, которые можно запускать на узле визуализатора по SSH или с помощью IPython notebook [8], предоставляющего веб-интерфейс для пользовательских задач и позволяющего просматривать результаты в браузере.
Разработанный программный код позволяет обрабатывать большие объемы данных в интересующие моменты времени и в выделенных зонах расчетной области. Также он позволяет параллельно с расчетами формировать дискретные кадры различных макрохарактеристик процесса и собирать их в видео-файл. Сбор данных производится по сети по протоколам SSH и SFTP, а так же посредством чтения из локальных директорий (или с использованием NFS или любых других способов). Также рассмотрены некоторые способы ускорения вычислений с помощью пакетов Numpy и Numba [9].
В результате применения программы при исследовании взаимодействия газа с металлической пластиной удалось в деталях наблюдать эффект адсорбции [3], который очень важен для многих практических приложений. В качестве иллюстрации работы программы на рис. 5, показан эффект адсорбции азота на поверхности никеля на выбранном пользователем участке. Для получения данного и других изображений, составляющих видеоролик об эволюции процесса адсорбции, пришлось обработать около 1 Тб распределенных данных, рассчитанных на МВС-10П (МСЦ РАН) и сохраненных на кластерах ИПМ им.М.В.Келдыша РАН и АО "НИЦЭВТ".
416
Труды ИСП РАН, том 27, вып. 6, 2015 г..
Рис. 5. Результат визуализации эффекта адсорбции азота на поверхности никеля.
Работа выполнена при поддержке Российского фонда фундаментальных исследований (проекты №№ 13-01-12073-офи_м, 15-07-06082-а, 15-29-07090-офи_м).
Список литературы
[1]	. Ю Н. Карамзин, Т.А. Кудряшова, В О. Подрыта, С.В. Поляков. Многомасштабное
моделирование нелинейных процессов в технических микросистемах. Математическое моделирование, 27(7), 2015. С. 65-74.
[2]	. В.О. Подрыта, С.В. Поляков, Д.В. Пузырьков. Суперкомпьютерное молекулярное
моделирование термодинамического равновесия в микросистемах газ-металл. Вычислительные методы и программирование, 16(1), 2015. С. 123-138.
[3]	. В.О. Подрыта, С.В. Поляков, В.В. Жаховский. Атомистический расчет перехода в
термодинамическое равновесие азота над поверхностью никеля. Математическое моделирование, 27(7), 2015. С. 91-96.
[4]	. А А. Бондаренко, С.В. Поляков, М.В. Якобовский, О.А. Косолапов, Э.М. Кононов.
Программный комплекс GIMM NANO. Международная суперкомпьютерная конференция "Научный сервис в сети Интернет: все грани параллелизма", 23 - 28 сентября 2013 г., г. Новороссийск, CD-proceedings, 1-5 рр.
[5]	. С.В. Поляков, Ю.Н. Карамзин, О.А. Косолапов, ТА. Кудряшова, С А. Суков.
Гибридная суперкомпьютерная платформа и разработка приложений для решения задач механики сплошной среды сеточными методами. Известия ЮФУ. Технические науки, № 6 (131), 2012. С. 105-115.
[6]	. SciPy official site —http://www.scipy.org/
[7]	. Mayavi official site — http://code.enthought.com/projects/mayavi/
[8]	. IPython official site — http://ipython.org/
[9]	. Numba official site — http://numba.pydata.org/
417
Trudy ISP RAN [The Proceedings of ISP RAS], vol. 27, issue 6, 2015.
Cloud service for decision of multiscale nanotechnology problems on supercomputer systems
S.	Polyakov <volvakov(a),imamod. ru>
A. Vyrodov <[email protected]>
D. Puzyrkov <[email protected]>
M. Yakobovskiy <lira(a),imamod.ru>
K1AMRAS, 4 Miusskaya square,
Moscow, 125047, Russian Federation
Abstract. In work the structure and separate components of the cloudy service intended for the decision of multi-scale problems of nanotechnology on supercomputer systems are presented. The need of integration: (a) an ideas and knowledge on this applied problem, (b) a specialists in this scientific field and a programmers for the supercomputer systems, (c) various technologies of modeling and a set of packages of applied programs, (d) various computing resources which are available for the Institute and its partners - was motivation to creation of cloudy service. The prototype of the cloudy environment realized in the form of service Multilogin and the applied software available on virtual machines of users became a results of this work. The first applications of created service are (a) the Flow_and_Particles parallel program for supercomputer calculations of multi-scale gasdynamics processes in micro-channels of technical systems and (b) the Flow_and_Particles_View visualizer of distributed computation results. Use of the developed service allowed to increase efficiency of scientific research in the chosen applied field.
Keywords:	cloud services and technologies; multiscale gasdynamics problems;
supercomputer simulations.
References
[1]	. Yu.N. Karamzin, T.A. Kudryashova, V.O. Podryga, S.V. Polyakov. Mnogomasshtabnoe
modelirovanie nelineynykh processov v tekhnicheskikh mikrosystemakh [Multiscale simulation of non-linear processes in technical micro-systems], Matematicheskoe modelirovanie [Mathematical Models and Computer Simulations], 2015, vol. 27, no. 7, pp. 65-74 (in Russian).
[2]	. V.O. Podryga, S.V. Polyakov, D.V. Puzyrkov. Superkompyutemoe molekulyamoe
modelirovanie termodinamicheskogo ravnovesiya v mikrosystemakh gas-metall [Supercomputer molecular simulation of thermodynamics equilibrium in micro-systems with gas and metal], Vychislitelnye metody i programmirovanie [Numerical Methods and Programming], vol. 16, no. 1,2015, pp. 123-138 (in Russian).
418
Труды ИСП РАН, том 27, вып. 6, 2015 г..
[3]	. V.O. Podryga, S.V. Polyakov, V.V. Zhakhovsky. Atomisticheskiy raschet perekhoda v
termodinamicheskoe ravnovesie azota nad poverkhnostyu nikelya [Atomistic calculation of transition to thermodynamic balance of nitrogen over a nickel surface], Matematicheskoe modelirovanie [Mathematical Models and Computer Simulations], 2015, vol. 27, no. 7, pp. 91-96 (in Russian).
[4]	. A.A. Bondarenko, S.V. Polyakov, M.V. Yakobovskiy, O.A. Kosolapov, EM. Kononov.
Programmnyi kompleks GIMM_NANO [The GIMMJSTANO software suite]. International Supercomputer Conference "Scientific service in Internet network: all sides of parallelism", 2013, September 23-28, Novorossiysk (Russia), CD-proceedings, pp. 333-337 (in Russian).
[5]	. S.V. Polyakov, Yu.N. Karamzin, O.A. Kosolapov, T.A. Kudryashova, S.A. Sukov.
Khibridnaya superkompyutemaya platforma i razrabotka prilozheniy dlya resheniya zadach mekhaniki sploshnoy sredy setochnymi metodami [Hybrid supercomputer platform and applications programming for the decision of continuum mechanics problems by grid methods], Izvestiya YuFU. Technicheskie nauki [News of the Southern Federal University. Technical Sciences], 2012, no. 6 (131), pp. 105-115 (in Russian).
[6]	. SciPy official site —http://www.scipy.org/
[7]	. Mayavi official site — http://code.enthought.com/projects/mayavi/
[8]	. IPython official site — http://ipython.org/
[9]	. Numba official site — http://numba.pydata.org/
419
